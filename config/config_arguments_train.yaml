name: Arguments configurations for SwinGPT

data_args:
  # Directories
  json_file_path_train: "/data/cad-recruit-02_814/swbaek/data_processed/combined/train_all.json"
  json_file_path_val: "/data/cad-recruit-02_814/swbaek/data_processed/combined/val_all.json"
  test_IC_path: "/data/cad-recruit-02_814/swbaek/data_processed/combined/val_IC.json"
  test_OD_path: "/data/cad-recruit-02_814/swbaek/data_processed/combined/val_OD.json"

  # Options
  image_aspect_ratio: "pad"
  lazy_preprocess: true
  is_multimodal: true
  is_eval_mode: false

model_args:
  # Directories
  model_name_or_path: "openai-community/gpt2"
  vision_tower: "microsoft/swin-tiny-patch4-window7-224"
  version: "swingpt_v0"
  checkpoint_path: "/data/cad-recruit-02_814/swbaek/submit/final/last.ckpt"

  # Options
  freeze_backbone: false
  tune_mm_mlp_adapter: false

  # Multimodal
  mm_use_images: true
  mm_use_bboxes: true
  mm_vision_select_layer: -2
  mm_vision_select_feature: "patch"
  mm_projector_type: "mlp2x_gelu"
  mm_patch_merge_type: "flat"
  pretrain_mm_mlp_adapter: null
  pretrained_mm_projector: null

  # Cross Attentions
  add_cross_attention: false

training_args:
  # Directories
  cache_dir: "/data/cad-recruit-02_814/swbaek/submit/final/"
  output_dir: "/data/cad-recruit-02_814/swbaek/submit/final/"

  # # Lora
  # lora_enable: true
  # lora_r: 128
  # lora_alpha: 256
  # mm_projector_lr: 0.00002

  # GPU training arguments
  strategy: "ddp" # Distributed data parallel
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false

  # Quantization
  quant_type: "nf4"
  bits: 16

  # Data type
  fp16: true

  # Optimizer & LR
  optim: "adamw_torch"
  mm_projector_lr: 0.00002
  learning_rate: 0.00002
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  
  # Data loading
  model_max_length: 1024
  dataloader_num_workers: 16
  num_train_epochs: 1
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  evaluation_strategy: "no"

  # Save
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 1
  report_to: "wandb"